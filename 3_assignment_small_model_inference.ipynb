{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Machine Learning Inference on the NPU with Pytorch and ONNX\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Demonstrate Ryzen SW flow for inference on NPU with pre-trained model\n",
    "\n",
    "* Learn Ryzen AI Software APIs\n",
    "    \n",
    "\n",
    "## Problem\n",
    "\n",
    "* Explore a trained model that classifies grayscale images of clothing items in the fashion-mnist dataset\n",
    "  \n",
    "* Use NPU to inference new images unseen during training process and classify per clothing item class\n",
    "\n",
    "      \n",
    "## References\n",
    "\n",
    "**[Ryzen AI SW repo](https://github.com/amd/RyzenAI-SW/tree/main/tutorial)**\n",
    "\n",
    "**[Ryzen AI Software Platform](https://ryzenai.docs.amd.com/en/latest/getstartex.html)**\n",
    "\n",
    "**[Vitis AI Execution Provider](https://onnxruntime.ai/docs/execution-providers/Vitis-AI-ExecutionProvider.html)**\n",
    "\n",
    "**[Guide on Quantization and Calibration](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)**\n",
    "\n",
    "**[Matplotlib Gallery](https://matplotlib.org/stable/gallery/axes_grid1/simple_axesgrid.html)**\n",
    "\n",
    "**[Fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)**\n",
    "\n",
    "**[Confusion Matrix](https://christianbernecker.medium.com/how-to-create-a-confusion-matrix-in-pytorch-38d06a7f04b7)**\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "<strong>The main objective of this lab is to introduce the Ryzen AI NPU APIs using a small dataset and smaller model. The power and performance benefits of the NPU are not covered in this assigment. PLease refer to demo notebooks shared in the repo as examples for the same</strong>\n",
    "<br><br>\n",
    "<strong>Notebook Runtime: ~10 minutes</strong>\n",
    "</div>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages\n",
    "\n",
    "Run the following cell to import all the necessary packages to be able to run the inference in the Ryzen AI NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import onnxruntime as ort\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import CalibrationDataReader, QuantType, QuantFormat, CalibrationMethod, quantize_static\n",
    "\n",
    "import quark\n",
    "from quark.onnx.quantization.config import (Config, get_default_config, QuantizationConfig)\n",
    "from quark.onnx import ModelQuantizer, PowerOfTwoMethod, QuantType\n",
    "from onnxruntime.quantization import CalibrationDataReader, QuantType, QuantFormat\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the data\n",
    "\n",
    "We'll a train model on the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. This consists of 70,000 grayscale images (28x28). Each image is associated with 1 of 10 classes. The dataset was split by the creators; there are 60,000 training images and 10,000 test images. \n",
    "\n",
    "Fashion MNIST classes:\n",
    "* T-shirt/top\n",
    "* Trouser\n",
    "* Pullover\n",
    "* Dress\n",
    "* Coat\n",
    "* Sandal\n",
    "* Shirt\n",
    "* Sneaker\n",
    "* Bag\n",
    "* Ankle boot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data for training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
    "                                                transforms.Compose([transforms.ToTensor()]))\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
    "                                               transforms.Compose([transforms.ToTensor()]))  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Partition a calibration set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-success\">\n",
    "<H3>ADD CODE IN BELOW CELL</H3>\n",
    "Split the full training set into a smaller 'train_set' and a 'calibration_set' ((e.g., 5,000 samples for calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## == ADD CODE HERE ==\n",
    "\n",
    "\n",
    "## ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                           batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                          batch_size=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SfsxOX4peHU"
   },
   "source": [
    "### Visualize FashionMNIST dataset classes. \n",
    "Making a method that return the name of class for the label number. e.g. if the label is 5, we return Sandal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWIE3hVqOlMi"
   },
   "outputs": [],
   "source": [
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "                 0: \"T-shirt/Top\",\n",
    "                 1: \"Trouser\",\n",
    "                 2: \"Pullover\",\n",
    "                 3: \"Dress\",\n",
    "                 4: \"Coat\", \n",
    "                 5: \"Sandal\", \n",
    "                 6: \"Shirt\",\n",
    "                 7: \"Sneaker\",\n",
    "                 8: \"Bag\",\n",
    "                 9: \"Ankle Boot\"\n",
    "                 }\n",
    "    input = (label.item() if type(label) == torch.Tensor else label)\n",
    "    return output_mapping[input]\n",
    "\n",
    "# Show 1 batch of train_set data\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "print(type(images), type(labels))\n",
    "print(images.shape, labels.shape)\n",
    "\n",
    "grid = torchvision.utils.make_grid(images, nrow=10)\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "print(\"labels: \", end=\" \")\n",
    "for i, label in enumerate(labels):\n",
    "    print(output_label(label), end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Building a CNN Model\n",
    "\n",
    "*   Make a model class\n",
    "    * It inherits nn.Module class that is a super class for all the neural networks in Pytorch.\n",
    "* The Neural Net has following layers:\n",
    "    * Two Sequential layers each consists of following layers-\n",
    "        * Convolution layer that has kernel size of 3 * 3, padding = 1 (zero_padding) in 1st layer and padding = 0 in second one. Stride of 1 in both layer.\n",
    "        * Batch Normalization layer.\n",
    "        * Acitvation function: ReLU.\n",
    "        * Max Pooling layer with kernel size of 2 * 2 and stride 2.\n",
    "     * Flatten out the output for dense layer(a.k.a. fully connected layer).\n",
    "     * 3 Fully connected layer  with different in/out features.\n",
    "     * 1 Dropout layer that has class probability p = 0.25.\n",
    "  \n",
    "     * All the functionaltiy is given in forward method that defines the forward pass of CNN.\n",
    "     * Our input image is changing in a following way:\n",
    "        * First Convulation layer : input: 28 \\* 28 \\* 3, output: 28 \\* 28 \\* 32\n",
    "        * First Max Pooling layer : input: 28 \\* 28 \\* 32, output: 14 \\* 14 \\* 32\n",
    "        * Second Conv layer : input : 14 \\* 14 \\* 32, output: 12 \\* 12 \\* 64\n",
    "        * Second Max Pooling layer : 12 \\* 12 \\* 64, output:  6 \\* 6 \\* 64\n",
    "    * Final fully connected layer has 10 output features for 10 types of clothes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters and loss function\n",
    "\n",
    "*  Creating a object(model in the code)\n",
    "*  Defining a Loss function. we're using CrossEntropyLoss() here.\n",
    "*  Using Adam algorithm for optimization purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionCNN()\n",
    "print(model)\n",
    "\n",
    "error = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-danger\">\n",
    "\n",
    "## Step 4 (Read-only): Training a network and testing it on test dataset\n",
    "#### We will not run the training here. The trained model file is given to you under onnx folder\n",
    "\n",
    "```python\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "error = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "count = 0\n",
    "# Lists for visualization of loss and accuracy \n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "# Lists for knowing classwise accuracy\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "    \n",
    "        train = Variable(images.view(100, 1, 28, 28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward pass \n",
    "        outputs = model(train)\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Initializing a gradient as 0 so there is no mixing of gradient among the batches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Propagating the error backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizing the parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "        count += 1\n",
    "    \n",
    "    # Testing the model\n",
    "    \n",
    "        if not (count % 50): \n",
    "            total = 0\n",
    "            correct = 0\n",
    "        \n",
    "            for images, labels in test_loader:\n",
    "                labels_list.append(labels)\n",
    "            \n",
    "                test = Variable(images.view(100, 1, 28, 28))\n",
    "            \n",
    "                outputs = model(test)\n",
    "            \n",
    "                predictions = torch.max(outputs, 1)[1]\n",
    "                predictions_list.append(predictions)\n",
    "                correct += (predictions == labels).sum()\n",
    "            \n",
    "                total += len(labels)\n",
    "            \n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        \n",
    "        if not (count % 500):\n",
    "            print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, loss.data, accuracy))\n",
    "```\n",
    "\n",
    "\n",
    "### Save PyTorch model\n",
    "After completing the training process, observe the following output:   \n",
    "* The trained fashion-mnist model is saved at the following location: `onnx/fashion-mnist.pt`.\n",
    "\n",
    "```python\n",
    "\n",
    "torch.save(model.state_dict(), 'onnx/fashion-mnist.pt')\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 5: ONNX model generation from a pre-trained pytorch model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('onnx/fashion-mnist.pt'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-success\">\n",
    "<H3>ADD CODE IN BELOW CELL</H3>\n",
    "Export the model to ONNX format and save to onnx folder 'onnx/fashion-mnist.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## == ADD CODE HERE ==\n",
    "\n",
    "\n",
    "\n",
    "## ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the input FP32 ONNX model\n",
    "\n",
    "Generated and adapted using Netron\n",
    ">Netron is a viewer for neural network, deep learning and machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "\n",
    "<strong>Note</strong> You can right click and download the file './onnx/fashion-mnist.onnx' from the file browser on the left and once the file is dowloaded to your local machine, run the cell below. Then open the model from your local machines' downloads folder.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "notebook_url = \"https://netron.app/\"\n",
    "\n",
    "iframe = IFrame(notebook_url, width=800, height=600)\n",
    "\n",
    "display(iframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Quantize the Model with Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantizing AI models from floating-point to 8-bit integers reduces computational power and the memory footprint required for inference. For model quantization, you can either use [AMD Quark](https://quark.docs.amd.com/latest/index.html) or [Microsoft Olive](https://ryzenai.docs.amd.com/en/latest/olive_quant.html). This example utilizes the AMD Quark quantizer workflow. \n",
    "   \n",
    "This will generate a quantized model using QDQ quant format and UInt8 activation type and Int8 weight type. After the run is completed, the quantized ONNX model `fashion-mnist.qdq.U8S8.onnx` is saved to `onnx/fashion-mnist.qdq.U8S8.onnx`.\n",
    "    \n",
    "For more information on representation of quantized ONNX models (e.g., QDQ quant format, UInt8 activation type and Int8 weight type) see [here](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html#onnx-quantization-representation-format)   \n",
    "   \n",
    "The  ```QuantizationConfig``` class is used to configure the quantization parameters to the model. \n",
    "\n",
    "#### Full List of Quantization Configuration Features can be [found here](https://quark.docs.amd.com/latest/onnx/appendix_full_quant_config_features.html)\n",
    "\n",
    "\n",
    "```python\n",
    "from quark.onnx import ModelQuantizer, PowerOfTwoMethod, QuantType\n",
    "from quark.onnx.quantization.config.config import Config, QuantizationConfig\n",
    "\n",
    "quant_config = QuantizationConfig(\n",
    "    quant_format=quark.onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=quark.onnx.PowerOfTwoMethod.MinMSE,\n",
    "    input_nodes=[],\n",
    "    output_nodes=[],\n",
    "    op_types_to_quantize=[],\n",
    "    per_channel=False,\n",
    "    reduce_range=False,\n",
    "    activation_type=quark.onnx.QuantType.QInt8,\n",
    "    weight_type=quark.onnx.QuantType.QInt8,\n",
    "    nodes_to_quantize=[],\n",
    "    nodes_to_exclude=[],\n",
    "    subgraphs_to_exclude=[],\n",
    "    optimize_model=True,\n",
    "    use_dynamic_quant=False,\n",
    "    use_external_data_format=False,\n",
    "    execution_providers=['CPUExecutionProvider'],\n",
    "    enable_npu_cnn=False,\n",
    "    enable_npu_transformer=False,\n",
    "    convert_fp16_to_fp32=False,\n",
    "    convert_nchw_to_nhwc=False,\n",
    "    include_cle=False,\n",
    "    include_sq=False,\n",
    "    extra_options={},)\n",
    "config = Config(global_quant_config=quant_config)\n",
    "\n",
    "quantizer = ModelQuantizer(config)\n",
    "quantizer.quantize_model(input_model_path, output_model_path, calibration_data_reader=None)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-success\">\n",
    "<H3>ADD CODE IN BELOW CELL</H3>\n",
    "Pass the 'calibration_set' and 'batch_size' to the Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmnistCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, batch_size: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## == ADD CODE HERE ==\n",
    "        \n",
    "        self.iterator = iter(torch.utils.data.DataLoader())  # Add code here\n",
    "        \n",
    "        ## ===================\n",
    "\n",
    "    def get_next(self) -> dict:\n",
    "        try:\n",
    "            images, labels = next(self.iterator)\n",
    "            return {\"input\": images.numpy()}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def fmnist_calibration_reader():\n",
    "    return FmnistCalibrationDataReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-success\">\n",
    "<H3>ADD CODE IN BELOW CELL</H3>\n",
    "<ul>\n",
    "<li>Set `input_model_path` - the path to the original, unquantized ONNX model.</li>\n",
    "<li>Set `output_model_path` - the path where the quantized model will be saved</li>\n",
    "<li>Set quantization configuration</li>\n",
    "<li>Create an ONNX quantizer and Quantize the ONNX model passing the fmnist_calibration_reader function we defined above</li>\n",
    "</ul>\n",
    "<br><b>Reference:  </b>2_pytorch_onnx_re-train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to quantize and save the model:\n",
    "\n",
    "## == ADD CODE HERE ==\n",
    "\n",
    "\n",
    "\n",
    "## ===================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the quantization process, observe the following output:\n",
    "\n",
    "* The quantized fashion-mnist model is saved at the following location in ONNX format: `onnx/fashion-mnist.qdq.U8S8.onnx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the INT8 quantized ONNX model\n",
    "\n",
    "Generated and adapted using Netron\n",
    ">Netron is a viewer for neural network, deep learning and machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "\n",
    "<strong>Note</strong> You can right click and download the file <b>'./onnx/fashion-mnist.qdq.U8S8.onnx'</b> from the file browser on the left and once the file is dowloaded to your local machine, run the cell below. Then open the model from your local machines' downloads folder.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "notebook_url = \"https://netron.app/\"\n",
    "\n",
    "iframe = IFrame(notebook_url, width=800, height=600)\n",
    "\n",
    "display(iframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Deploy the Model on NPU for Inference with calibration\n",
    "### Setup xclbin for NPU\n",
    "Run the next cell to set up the XLNX_VART_FIRMWARE environmental variable to point to the NPU binary. The NPU binary is an AI design that provides up to high performance. multiple such AI streams can be run in parallel on the NPU without any visible loss of performance.<br>\n",
    "<span style=\"color:chocolate\"><b>Note:</b></span> `XLNX_VART_FIRMWARE` Env variable  is set to the `xclbin` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIE array \n",
    "os.environ['XLNX_ENABLE_CACHE'] = '0'\n",
    "os.environ['XLNX_VART_FIRMWARE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load quantized ONNX model\n",
    "\n",
    "Run the following cell to load the provided ONNX quantized model.\n",
    "\n",
    "<div class=\"alert alert-box alert-info\">\n",
    "\n",
    "We will use the following pre-trained quantized file:\n",
    "\n",
    "* The trained quantized fashion-mnist model is saved at the following location: `onnx/fashion-mnist.qdq.U8S8.onnx`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path1 = r'onnx/fashion-mnist.qdq.U8S8.onnx'\n",
    "model = onnx.load(quantized_model_path1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-success\">\n",
    "<H3>ADD CODE IN BELOW CELL</H3>\n",
    "Deploy model using Vitis AI EP<br>\n",
    "<br><b>Reference:</b>\n",
    "<br><a>- https://ryzenai.docs.amd.com/en/latest/modelrun.html</a></br>\n",
    "- 1_pytorch_onnx_inference-NPU.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## == ADD CODE HERE ==\n",
    "\n",
    "\n",
    "## ===================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_opt = ort.RunOptions()\n",
    "input_name = session.get_inputs()[0].name\n",
    "label_name = session.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference on Test set\n",
    "\n",
    "20 images from the test set are read, classified and  visualized by running the quantized fashion-mnist model on the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_images = 20  # Display 20 images\n",
    "num_cols = 5\n",
    "\n",
    "fig, axs = plt.subplots(num_images // num_cols, num_cols, figsize=(10, 10))\n",
    "subplot_size = (8, 8)\n",
    "\n",
    "cm_predicted_labels = []\n",
    "cm_actual_labels = []\n",
    "misclassified_images = []\n",
    "misclassified_labels = []\n",
    "\n",
    "for i in range(num_images):\n",
    "    img, label = test_set[i]\n",
    "    cm_actual_labels.append(output_label(label))\n",
    "    img = img[np.newaxis, :]\n",
    "    \n",
    "    outputs = session.run(None, {input_name: img.numpy()}, run_options=run_opt)\n",
    "    logits = torch.tensor(outputs)\n",
    "\n",
    "    softmax_probs = torch.softmax(logits[0], dim=1)\n",
    "    predicted_class = torch.argmax(softmax_probs, dim=1).item()\n",
    "    cm_predicted_labels.append(output_label(predicted_class))\n",
    "\n",
    "     # Process the outputs\n",
    "    if (label != predicted_class):\n",
    "        misclassified_images.append(i)\n",
    "        misclassified_labels.append(output_label(predicted_class))\n",
    "\n",
    "    axs[i // num_cols, i % num_cols].imshow(img.numpy().squeeze(), cmap=\"PRGn\")\n",
    "    axs[i // num_cols, i % num_cols].set_title(f\"\"\"Image {i} \n",
    "    Predicted class: {output_label(predicted_class)}\"\"\", fontsize=8)\n",
    "    axs[i // num_cols, i % num_cols].axis('off')\n",
    "    axs[i // num_cols, i % num_cols].set_aspect('auto')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display any Mis-classifications\n",
    "\n",
    "20 images from the test set are read, classified and  visualized by running the quantized fashion-mnist model on the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total misclassified images: {len(misclassified_images)}\")\n",
    "print(misclassified_images)\n",
    "\n",
    "show_imlist_mis = []\n",
    "\n",
    "for i in misclassified_images:\n",
    "    show_imlist_mis.append(test_set[i][0].squeeze())\n",
    "\n",
    "varpltsize = len(misclassified_images)\n",
    "\n",
    "fig = plt.figure(figsize=((1 * 2 * varpltsize), 1 * 2 * varpltsize))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(1, len(misclassified_images)),  \n",
    "                 axes_pad=0.3,  # pad between axes in inch.\n",
    "                 )\n",
    "cnt=0\n",
    "for ax, image, label in zip(grid, show_imlist_mis, misclassified_labels):\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(image,cmap=\"PRGn\")\n",
    "    ax.set_title(f\"\"\"Predicted label: {label} \n",
    "    Actual label: {output_label(test_set[misclassified_images[cnt]][1])}\"\"\", fontdict={'fontsize':8})\n",
    "    cnt+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "<H4>Below cells will check the correct working of the notebook.</H4>\n",
    "<ul>\n",
    "<li>Accuracy of model on test_set should be above 85%</li>\n",
    "<li>Reasonable confusion Matrix</li>\n",
    "<li>36 Nodes should run on the NPU in the vitisai_ep_report.json ouput JSON file</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure accuracy on entire test set\n",
    "\n",
    "10000 images from the test set are read, classified and we calculate the accuracy of the quantized fashion-mnist model on the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_images = 10000 \n",
    "num_cols = 5\n",
    "\n",
    "cm_predicted_labels = []\n",
    "cm_actual_labels = []\n",
    "misclassified_images = []\n",
    "misclassified_labels = []\n",
    "\n",
    "for i in range(num_images):\n",
    "    img, label = test_set[i]\n",
    "    cm_actual_labels.append(output_label(label))\n",
    "    img = img[np.newaxis, :]\n",
    "    \n",
    "    outputs = session.run(None, {input_name: img.numpy()}, run_options=run_opt)\n",
    "    logits = torch.tensor(outputs)\n",
    "\n",
    "    softmax_probs = torch.softmax(logits[0], dim=1)\n",
    "    predicted_class = torch.argmax(softmax_probs, dim=1).item()\n",
    "    cm_predicted_labels.append(output_label(predicted_class))\n",
    "\n",
    "    # Process the outputs\n",
    "    if (label != predicted_class):\n",
    "        misclassified_images.append(i)\n",
    "        misclassified_labels.append(output_label(predicted_class))\n",
    "\n",
    "print(f\"Total correctly classified images: {10000 - len(misclassified_images)}\")\n",
    "print(f\"Total misclassified images: {len(misclassified_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-info\">\n",
    "The below accuracy on the test images is calculated for the quantized model run on the NPU.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Accuracy of the quantized model for the test set is : {(accuracy_score(cm_actual_labels, cm_predicted_labels)*100):.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "The X-axis represents the predicted class and the Y-axis represents the actual class.\n",
    "\n",
    "The diagonal cells show true positives, they show how many instances of each class were correctly predicted by the model. \n",
    "The off-diagonal cells show instances where the predicted class did not match the actual class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(cm_actual_labels, cm_predicted_labels)\n",
    "df = pd.DataFrame(cf_matrix/np.sum(cf_matrix,axis=1), index = [output_label(i) for i in range(10)], columns=[output_label(i) for i in range(10)])\n",
    "plt.figure(figsize = (10,5));\n",
    "sn.heatmap(df, annot=True, cmap=\"PiYG\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Output JSON file and analyze partitioning\n",
    "Analyze operator partitions between the NPU and CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "import json\n",
    "\n",
    "# Load JSON from file\n",
    "with open(\"./onnx/modelcachekey_lab4/vitisai_ep_report.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Display nicely in Jupyter\n",
    "JSON(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-info\">\n",
    "<b>Note: Modify the \"modelcachekey\" folder name incase you are using a different folder name </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = os.path.join(os.getcwd(), \"onnx\", \"modelcachekey\",\"*\")  \n",
    "files = glob.glob(cache_path)\n",
    "for f in files:\n",
    "    try:\n",
    "        os.remove(f)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training an Image Classification Model in PyTorch",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
