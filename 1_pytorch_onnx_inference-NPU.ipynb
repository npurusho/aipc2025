{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Machine Learning Inference on the NPU with Pytorch and ONNX\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Introduce the Ryzen™ AI Software Platform\n",
    "\n",
    "* Show the ONNX model generation and inference flow on the NPU\n",
    "    \n",
    "* Deploy a quantized ResNet-50 model onto Ryzen AI NPU for inference\n",
    "      \n",
    "## References\n",
    "\n",
    "**[Ryzen AI SW repo](https://github.com/amd/RyzenAI-SW/tree/main/tutorial)**\n",
    "\n",
    "**[Ryzen AI Software Platform](https://ryzenai.docs.amd.com/en/latest/getstartex.html)**\n",
    "\n",
    "**[Vitis AI Execution Provider](https://onnxruntime.ai/docs/execution-providers/Vitis-AI-ExecutionProvider.html)**\n",
    "\n",
    "**[Guide on Quantization and Calibration](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)**\n",
    "\n",
    "**[Matplotlib Gallery](https://matplotlib.org/stable/gallery/axes_grid1/simple_axesgrid.html)**\n",
    "\n",
    "**[CIFAR10](https://github.com/EN10/CIFAR)**\n",
    "\n",
    "**[Confusion Matrix](https://christianbernecker.medium.com/how-to-create-a-confusion-matrix-in-pytorch-38d06a7f04b7)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ryzen AI Software Platform\n",
    "\n",
    "The AMD Ryzen™ AI Software Platform enables developers to take machine learning models trained in PyTorch or TensorFlow and run them on laptops powered by Ryzen AI. The Ryzen AI software platform intelligently optimizes tasks and workloads, freeing-up CPU and GPU resources, and ensuring optimal performance at lower power. The diagram below shows the flow from trained models to execution.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/raisw13.png\" alt=\"sdk\" style=\"max-height: 400px; width:auto; height:auto;\">\n",
    "</center>\n",
    "<center><strong>Ryzen AI software platform</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages\n",
    "\n",
    "Run the following cell to import all the necessary packages to be able to run the inference in the Ryzen AI NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Vision and Image Processing Libraries\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "# Torch and torchvision libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# ONNX and Quantization Libraries\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization.calibrate import CalibrationDataReader\n",
    "from onnxruntime.quantization import CalibrationDataReader, QuantType, QuantFormat, CalibrationMethod, quantize_static\n",
    "from quark.onnx.quantization.config import Config, get_default_config\n",
    "from quark.onnx import ModelQuantizer\n",
    "\n",
    "# General Libraries\n",
    "import enum\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import time\n",
    "\n",
    "# Data Science and Metrics Libraries\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a pre-trained ResNet-50 model from PyTorch Hub for the CIFAR-10 dataset.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the CIFAR-10 dataset\n",
    "\n",
    "Execute the following cells to download the CIFAR-10 dataset. The dataset is stored in `data/cifar-10-batches-py/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global models_dir, data_dir\n",
    "models_dir = \".\\\\onnx\"\n",
    "data_dir= \".\\\\onnx\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License 1 (see end of notebook)\n",
    "\n",
    "# Download data - One-time only\n",
    "\n",
    "datadirname = \".\\\\onnx\\\\data\"\n",
    "if not os.path.exists(datadirname):\n",
    "   data_download_tar = \"cifar-10-python.tar.gz\"\n",
    "   urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", data_download_tar)\n",
    "   file = tarfile.open(data_download_tar)\n",
    "   file.extractall(data_dir)\n",
    "   file.close()\n",
    "\n",
    "# # Delete cifar-10-python.tar.gz source file after all images are extracted\n",
    "# data_images_path = os.path.join(os.getcwd(), \"cifar-10-python.tar.gz\")\n",
    "# files = glob.glob(data_images_path)\n",
    "# for f in files:\n",
    "#     os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [CIFAR-10](https://github.com/EN10/CIFAR) dataset has 60,000 32x32 pixels color images in 10 classes, each class consists of 6,000 images.\n",
    "There are 50,000 training images and 10,000 test images.   \n",
    "The dataset contains five training batches and one test batch, 10,000 images in each. Each class in the test batch has 1,000 randomly selected images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-info\">\n",
    "For inference we use the 10,000 test images.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR10 classes are enumerated in the `Cifar10Classes` class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Classes(enum.Enum):\n",
    "    airplane = 0\n",
    "    automobile = 1\n",
    "    bird = 2\n",
    "    cat = 3\n",
    "    deer = 4\n",
    "    dog = 5\n",
    "    frog = 6\n",
    "    horse = 7\n",
    "    ship = 8\n",
    "    truck = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following two cells to display a subset of the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License 2 (see end of notebook)\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file,'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict\n",
    "\n",
    "datafile = r'./onnx/data/cifar-10-batches-py/test_batch'\n",
    "metafile = r'./onnx/data/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "test_batch = unpickle(datafile) \n",
    "metadata = unpickle(metafile)\n",
    "\n",
    "images = test_batch['data']\n",
    "labels = test_batch['labels']\n",
    "images = np.reshape(images,(10000, 3, 32, 32))\n",
    "\n",
    "im = []\n",
    "\n",
    "dirname = 'onnx/onnx_test_images'\n",
    "if not os.path.exists(dirname):\n",
    "   os.mkdir(dirname)\n",
    "\n",
    "for i in range(20):\n",
    "    im.append(cv2.cvtColor(images[i].transpose(1,2,0), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(4, 5),  # creates 4x5 grid of axes\n",
    "                 axes_pad=0.5,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, image, label in zip(grid, im, labels):\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f'Actual label: {Cifar10Classes(label).name}', fontdict={'fontsize':8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Deploy the Model on the NPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to check the `XLNX_VART_FIRMWARE` environmental variable is pointing to the right NPU binary. The NPU binary `AMD_AIE2P_Nx4_Overlay.xclbin` is an AI design that provides high performance/watt. Multiple such AI streams can be run in parallel on the NPU without any visible loss of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['XLNX_VART_FIRMWARE']=\"C:\\\\Program Files\\\\RyzenAI\\\\1.3.1\\\\voe-4.0-win_amd64\\\\xclbins\\\\AMD_AIE2P_Nx4_Overlay.xclbin\"\n",
    "os.environ['XLNX_VART_FIRMWARE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the ONNX model\n",
    "\n",
    "Generated and adapted using Netron\n",
    ">Netron is a viewer for neural network, deep learning and machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "\n",
    "<strong>Note</strong> You can right click and download the file './onnx/resent.qdq.U8S8.onnx' from the file browser on the left and once the file is dowloaded to your local machine, run the cell below. Then open the model from your local machines' downloads folder.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "notebook_url = \"https://netron.app/\"\n",
    "\n",
    "iframe = IFrame(notebook_url, width=800, height=600)\n",
    "\n",
    "display(iframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load quantized ONNX model\n",
    "\n",
    "Run the following cell to load the provided ONNX quantized model.\n",
    "\n",
    "<div class=\"alert alert-box alert-info\">\n",
    "\n",
    "We will use the following pre-trained quantized file:\n",
    "\n",
    "* The trained quantized ResNet-50 model on the CIFAR-10 dataset is saved at the following location: `onnx/resnet.qdq.U8S8.onnx`\n",
    "\n",
    "If you would like to re-train and quantize your model, please review the [Pytorch_ONNX_re-train](5_2_pytorch_onnx_re-train.ipynb) notebook.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License 2 (see end of notebook)\n",
    "\n",
    "quantized_model_path = r'./onnx/resnet.qdq.U8S8.onnx'\n",
    "model = onnx.load(quantized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the quantized ONNX model on the Ryzen AI NPU\n",
    "\n",
    "For more information on provider options visit [ONNX Runtime with Vitis AI Execution Provider](https://ryzenai.docs.amd.com/en/latest/modelrun.html)\n",
    "\n",
    "<div class=\"alert alert-box alert-info\">\n",
    "\n",
    "The file `onnx/vaip_config.json` is required when configuring Vitis AI Execution Provider (VAI EP) inside the ONNX Runtime code.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License 2 (see end of notebook)\n",
    "\n",
    "providers = ['VitisAIExecutionProvider']\n",
    "cache_dir = os.path.join(os.getcwd(), \"onnx\")\n",
    "provider_options = [{\n",
    "            'config_file': 'onnx/vaip_config.json',\n",
    "            'cacheDir': str(cache_dir),\n",
    "            'cacheKey': 'modelcachekey'\n",
    "        }]\n",
    "\n",
    "session = onnxruntime.InferenceSession(model.SerializeToString(), providers=providers,\n",
    "                               provider_options=provider_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "The first 20 images are extracted from the CIFAR-10 test dataset and converted to the .png format.\n",
    "\n",
    "The .png images are read, classified and visualized by running the quantized ResNet-50 model on the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# License 2 (see end of notebook)\n",
    "\n",
    "# Extract and dump first 20 images \n",
    "for i in range(20): \n",
    "    im = images[i]\n",
    "    im  = im.transpose(1,2,0)\n",
    "    im = cv2.cvtColor(im,cv2.COLOR_RGB2BGR)\n",
    "    im_name = f'./{dirname}/image_{i}.png'\n",
    "    cv2.imwrite(im_name, im)\n",
    "\n",
    "viz_predicted_labels = []\n",
    "misclassified_images = []\n",
    "misclassified_labels = []\n",
    "show_imlist = []\n",
    "\n",
    "# Pick dumped images and predict\n",
    "for i in range(20): \n",
    "    image_name = f'./{dirname}/image_{i}.png'\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    # Resize the image to match the input size expected by the model\n",
    "    image = image.resize((32, 32))  \n",
    "    image_array = np.array(image).astype(np.float32)\n",
    "    image_array = image_array/255\n",
    "\n",
    "    # Reshape the array to match the input shape expected by the model\n",
    "    image_array = np.transpose(image_array, (2, 0, 1))  \n",
    "\n",
    "    # Add a batch dimension to the input image\n",
    "    input_data = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Run the model\n",
    "    outputs = session.run(None, {'input': input_data})\n",
    "\n",
    "    # Process the outputs\n",
    "    predicted_class = np.argmax(outputs[0])\n",
    "    predicted_label = metadata['label_names'][predicted_class]\n",
    "    viz_predicted_labels.append(predicted_class)\n",
    "    label = metadata['label_names'][labels[i]]\n",
    "    # print(f'Image {i}: Actual Label {label}, Predicted Label {predicted_label}')\n",
    "    if (label != predicted_label):\n",
    "        misclassified_images.append(i)\n",
    "        misclassified_labels.append(predicted_label)\n",
    "\n",
    "    show_imlist.append(cv2.cvtColor(images[i].transpose(1,2,0), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(4, 5),  # creates 4x5 grid of axes\n",
    "                 axes_pad=0.3,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, image, label in zip(grid, show_imlist, viz_predicted_labels):\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f'Predicted label: {Cifar10Classes(label).name}', fontdict={'fontsize':8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_imlist_mis = []\n",
    "\n",
    "for i in misclassified_images:\n",
    "    show_imlist_mis.append(cv2.cvtColor(images[i].transpose(1,2,0), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "varpltsize = len(misclassified_images)\n",
    "\n",
    "fig = plt.figure(figsize=((1 * 2 * varpltsize), 1 * 2 * varpltsize))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(1, len(misclassified_images)),  \n",
    "                 axes_pad=0.3,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, image, label in zip(grid, show_imlist_mis, misclassified_labels):\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f'Predicted label: {label}', fontdict={'fontsize':8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for more test images\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "<strong>Note:</strong> the cell below may extract up to 5,000 images. You can delete the extracted images by following the instructions in <b>Delete all Extracted Images</b>.\n",
    "</div>\n",
    "\n",
    "The first 5,000 images are extracted from the CIFAR-10 test dataset and converted to the .png format.    \n",
    "The .png images are read, classified and visualized by running the quantized ResNet-50 model on the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License 2 (see end of notebook)\n",
    "\n",
    "\n",
    "max_images = len(images)//2 # 5000 test images\n",
    "\n",
    "# Extract and dump all images in the test set \n",
    "for i in range(max_images): \n",
    "    im = images[i]\n",
    "    im  = im.transpose(1,2,0)\n",
    "    im = cv2.cvtColor(im,cv2.COLOR_RGB2BGR)\n",
    "    im_name = f'./{dirname}/image_{i}.png'\n",
    "    cv2.imwrite(im_name, im)\n",
    "\n",
    "cm_predicted_labels = []\n",
    "cm_actual_labels = []\n",
    "\n",
    "print('----------------------------------------')\n",
    "print(\"Status\")\n",
    "print('----------------------------------------')\n",
    "\n",
    "# Pick dumped images and predict\n",
    "for i in range(max_images): \n",
    "    image_name = f'./{dirname}/image_{i}.png'\n",
    "    try:\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "    except:\n",
    "        print(f\"Warning: Image {image_name} maybe locked moving on to next image\")\n",
    "        continue\n",
    "    # Resize the image to match the input size expected by the model\n",
    "    image = image.resize((32, 32))  \n",
    "    image_array = np.array(image).astype(np.float32)\n",
    "    image_array = image_array/255\n",
    "\n",
    "    # Reshape the array to match the input shape expected by the model\n",
    "    image_array = np.transpose(image_array, (2, 0, 1))  \n",
    "\n",
    "    # Add a batch dimension to the input image\n",
    "    input_data = np.expand_dims(image_array, axis=0)\n",
    "    # Run the model\n",
    "    outputs = session.run(None, {'input': input_data})\n",
    "\n",
    "    # Process the outputs\n",
    "    predicted_class = np.argmax(outputs[0])\n",
    "    predicted_label = metadata['label_names'][predicted_class]\n",
    "    cm_predicted_labels.append(predicted_class)\n",
    "    label = metadata['label_names'][labels[i]]\n",
    "    cm_actual_labels.append(labels[i])\n",
    "    if i%990 == 0:\n",
    "        print(f'Status: Running Inference on image {i}... Actual Label: {label}, Predicted Label: {predicted_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "The X-axis represents the predicted class and the Y-axis represents the actual class.\n",
    "\n",
    "The diagonal cells show true positives, they show how many instances of each class were correctly predicted by the model. \n",
    "The off-diagonal cells show instances where the predicted class did not match the actual class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(cm_actual_labels, cm_predicted_labels)\n",
    "df = pd.DataFrame(cf_matrix/np.sum(cf_matrix,axis=1), index = [Cifar10Classes(i).name for i in range(10)], columns=[Cifar10Classes(i).name for i in range(10)])\n",
    "plt.figure(figsize = (10,5));\n",
    "sn.heatmap(df, annot=True, cmap=\"PiYG\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the quantized model for 5,000 test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-info\">\n",
    "The below accuracy on the test images is calculated for the quantized model run on the NPU.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Accuracy of the quantized model for the test set is : {(accuracy_score(cm_actual_labels, cm_predicted_labels)*100):.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Output json file and analyze partitioning\n",
    "Analyze operator partitions between the NPU and CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "import json\n",
    "\n",
    "# Load JSON from file\n",
    "with open(\"./onnx/modelcachekey/vitisai_ep_report.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Display nicely in Jupyter\n",
    "JSON(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete all Extracted Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all extracted images to save disk space \n",
    "images_path = os.path.join(os.getcwd(), \"onnx\", \"onnx_test_images\",\"*\")\n",
    "files = glob.glob(images_path)\n",
    "for f in files:\n",
    "    try:\n",
    "        os.remove(f)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License 1\n",
    "\n",
    "```python\n",
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "# --------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License 2\n",
    "\n",
    "```python\n",
    "#################################################################################  \n",
    "# License\n",
    "# Ryzen AI is licensed under `MIT License <https://github.com/amd/ryzen-ai-documentation/blob/main/License>`_ . Refer to the `LICENSE File <https://github.com/amd/ryzen-ai-documentation/blob/main/License>`_ for the full license text and copyright notice.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "<center>\n",
    "Copyright&copy; 2025 AMD, Inc\n",
    "</center>\n",
    "<center>\n",
    "SPDX-License-Identifier: MIT\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
