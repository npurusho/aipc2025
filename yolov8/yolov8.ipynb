{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with video file on Ryzen AI\n",
    "\n",
    "This example demonstrates the object detection model inference on the embedded Neural Processing Unit (NPU) in your AMD Ryzen AI enabled PC with either single image or the live webcam. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Before starting, be sure you've installed the requirements listed in the requirements.txt file:\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Model from Ryzen AI model zoo\n",
    "The yolov8 model from [Ryzen AI model zoo](https://huggingface.co/amd) will be applied in this example. You may choose any other object detection models with tiny difference in the pre and post processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58468aa9af9e44afa8d68f580ef3a5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "yolov8m.onnx:   0%|          | 0.00/104M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\AUP\\\\npucloud_userdata\\\\mario-test\\\\ryzenaisw\\\\iisc_eo294_npu_assignment\\\\yolov8\\\\yolov8m.onnx'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notebook dependencies\n",
    "from huggingface_hub import hf_hub_download\n",
    "from yolov8_utils import get_directories\n",
    "\n",
    "current_dir = get_directories()\n",
    "\n",
    "# Download Yolov8 model from Ryzen AI model zoo. Registration is required before download.\n",
    "hf_hub_download(repo_id=\"amd/yolov8m\", filename=\"yolov8m.onnx\", local_dir=str(current_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model inference on NPU with webcam\n",
    "\n",
    "Now we have validated the the model with image., and we will use the webcam as live input to do the inference on NPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-info\">\n",
    "<h3>Note:</h3> \n",
    "The below cell uses a video with filename <b>example.mp4</b>, this video is <b>NOT</b> provided for you due to license issue. <br>\n",
    "Please use your own video here and test the model, you can name it example.mp4.<br>\n",
    "You can download free license videos from here: <a><b> https://pixabay.com/videos/search/traffic%20india/ </b></a> <br>\n",
    "Make sure the video clip is small of about a few seconds<br>\n",
    "You can download the video to your local machine and drag and drop it in the file browser on the left in Jupyter.\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import cv2\n",
    "from yolov8_utils import *\n",
    "\n",
    "# display videos in notebook\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "\n",
    "def frame_process(frame, input_shape=(640, 640)):\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, input_shape)\n",
    "    img = torch.from_numpy(img)\n",
    "    img = img.float()  # uint8 to fp16/32\n",
    "    img /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return img\n",
    "    \n",
    "\n",
    "class DFL(nn.Module):\n",
    "    # Integral module of Distribution Focal Loss (DFL) proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "    def __init__(self, c1=16):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(c1, dtype=torch.float)\n",
    "        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))\n",
    "        self.c1 = c1\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, a = x.shape  # batch, channels, anchors\n",
    "        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(\n",
    "            b, 4, a\n",
    "        )\n",
    "\n",
    "\n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    lt, rb = torch.split(distance, 2, dim)\n",
    "    x1y1 = anchor_points - lt\n",
    "    x2y2 = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy = (x1y1 + x2y2) / 2\n",
    "        wh = x2y2 - x1y1\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n",
    "\n",
    "def post_process(x):\n",
    "    dfl = DFL(16)\n",
    "    anchors = torch.tensor(\n",
    "        np.load(\n",
    "            \"./anchors.npy\",\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "    )\n",
    "    strides = torch.tensor(\n",
    "        np.load(\n",
    "            \"./strides.npy\",\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "    )\n",
    "    box, cls = torch.cat([xi.view(x[0].shape[0], 144, -1) for xi in x], 2).split(\n",
    "        (16 * 4, 80), 1\n",
    "    )\n",
    "    dbox = dist2bbox(dfl(box), anchors.unsqueeze(0), xywh=True, dim=1) * strides\n",
    "    y = torch.cat((dbox, cls.sigmoid()), 1)\n",
    "    return y, x\n",
    "\n",
    "def inference(ep):\n",
    "    # Load labels of coco dataaset\n",
    "    with open('coco.names', 'r') as f:\n",
    "            names = f.read()\n",
    "    \n",
    "    imgsz = [640, 640]\n",
    "    \n",
    "    with open('coco.names', 'r') as f:\n",
    "            names = f.read()\n",
    "    \n",
    "    # Video input\n",
    "    # cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Video input from local file\n",
    "    \n",
    "    # Get the current working directory\n",
    "    output_path = os.getcwd()\n",
    "    video_path = output_path+\"\\\\example.mp4\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    inference_time=0.0\n",
    "    total_inference_time=0.0\n",
    "    \n",
    "    while (True):\n",
    "        try:\n",
    "            clear_output(wait=True)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            input_shape = (640, 640)\n",
    "    \n",
    "            im = frame_process(frame, input_shape)\n",
    "            if len(im.shape) == 3:\n",
    "                im = im[None]\n",
    "    \n",
    "            start = time.time()\n",
    "            outputs = ep.run(None, {ep.get_inputs()[0].name: im.permute(0, 2, 3, 1).cpu().numpy()})\n",
    "            end = time.time()\n",
    "    \n",
    "            inference_time = np.round((end - start) * 1000, 2)\n",
    "    \n",
    "            total_inference_time +=inference_time\n",
    "            \n",
    "            # Postprocessing\n",
    "            outputs = [torch.tensor(item).permute(0, 3, 1, 2) for item in outputs]\n",
    "            preds = post_process(outputs)\n",
    "            preds = non_max_suppression(\n",
    "                preds, 0.25, 0.7, agnostic=False, max_det=300, classes=None\n",
    "            )\n",
    "    \n",
    "            colors = [[random.randint(0, 255) for _ in range(3)] \n",
    "                    for _ in range(len(names))]\n",
    "    \n",
    "            plot_images(\n",
    "            im,\n",
    "            *output_to_target(preds, max_det=15),\n",
    "            frame,\n",
    "            fname=\"output.jpg\",\n",
    "            names=names,\n",
    "            )\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            cap.release()\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    print('Inference time: ' + str(total_inference_time) + \" ms\")\n",
    "    print('----------------------------------------')\n",
    "\n",
    "# Specify the path to the quantized ONNZ Model\n",
    "onnx_model_path = \"yolov8m.onnx\"\n",
    "\n",
    "# Point to the config file path used for the VitisAI Execution Provider\n",
    "config_file_path = \"./vaip_config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Inference time: 22804.460000000006 ms\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cpu_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the CPU\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "\n",
    "inference(cpu_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Inference time: 9484.139999999998 ms\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dml_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the iGPU\n",
    "dml_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['DmlExecutionProvider'],\n",
    "    provider_options = [{\"device_id\": \"0\"}]\n",
    ")\n",
    "\n",
    "inference(dml_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Inference time: 2903.2799999999997 ms\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "npu_options = onnxruntime.SessionOptions()\n",
    "\n",
    "npu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['VitisAIExecutionProvider'],\n",
    "    sess_options=npu_options,\n",
    "    provider_options=[{'config_file': config_file_path}]\n",
    ")\n",
    "\n",
    "inference(npu_session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
